
<!DOCTYPE html>
<html lang="en">

  <head>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BR0EX3ZS1G"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-BR0EX3ZS1G');
    </script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;400;500;700;900&display=swap" rel="stylesheet">

    <title>Nupur Iyer - Welcome to my world!</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

<!--

  Nupur Iyer portfolio
  Inspired by the TemplateMo 570 Chain App Dev
  https://templatemo.com/tm-570-chain-app-dev

-->

    <!-- Additional CSS Files -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <link rel="stylesheet" href="assets/css/templatemo-chain-app-dev.css">
    <link rel="stylesheet" href="assets/css/animated.css">
    <link rel="stylesheet" href="assets/css/owl.css">

  </head>

<body>

  <!-- ***** Preloader Start ***** -->
  <div id="js-preloader" class="js-preloader">
    <div class="preloader-inner">
      <span class="dot"></span>
      <div class="dots">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
  </div>
  <!-- ***** Preloader End ***** -->
  <!-- ***** Header Area Start ***** -->
  <header class="header-area header-sticky wow slideInDown" data-wow-duration="0.75s" data-wow-delay="0s">
    <div class="container">
      <div class="row">
        <div class="col-12">
          <nav class="main-nav">
            <!-- ***** Logo Start ***** -->
            <a href="index.html" class="logo">
              <img src="assets/images/logo v2.png" alt="Nupur Iyer a.k.a. me!">
            </a>
            <!-- ***** Logo End ***** -->

            <!-- ***** Menu Start ***** -->
            <ul class="nav">
              <li class="scroll-to-section"><a href="index.html">Home</a></li>
              <li class="scroll-to-section"><a href="index.html">About</a></li>
              <li class="scroll-to-section"><a href="#projects" class="active">Projects</a></li>
              <li class="scroll-to-section"><a href="certificates.html">Certifications</a></li>
              <li class="scroll-to-section"><a href="index.html">Reviews</a></li>
              <li><div class="gradient-button"><a id="modal_trigger" href="assets/pdf/NUPUR IYER RESUME.pdf"><i class="fa fa-sign-in-alt"></i> Download my Resume</a></div></li> 
            </ul>        
            <a class='menu-trigger'>
                <span>Menu</span>
            </a>
            <!-- ***** Menu End ***** -->
          </nav>
        </div>
      </div>
    </div>
  </header>
  <!-- ***** Header Area End ***** -->    
</div>

<div id="projects" class="services section">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 offset-lg-2">
          <div class="section-heading  wow fadeInDown" data-wow-duration="1s" data-wow-delay="0.5s">
            <h4>Some of my work <em></em></h4>
            <img src="assets/images/heading-line-dec.png" alt="">
            <p>With curiosity steering the ship and my fascination with AI and its implications on the human mind, I've completed numerous projects in the AI space. The following lists some of my top work.</p>
          </div>
        </div>
      </div>
    </div>
    <div class="container">
      <div class="row">
        <div class="col-lg-3">
          <div class="service-item first-service">
            <div class="icon"></div>
            <h4>Conversations with an AI: Psych Eval of ChatGPT</h4>
            <p>aims to study the psychological side of the AI in question (ChatGPT) via conversations targeted towards what the AI “inherently knows.”</p>
            <div class="text-button">
              <a href="https://medium.com/@nupuriyer/conversations-with-an-ai-958311d848da">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="nlpsa" class="col-lg-3">
          <div class="service-item second-service">
            <div class="icon"></div>
            <h4>Sentiment Analysis with NLP</h4>
            <p>Using NLP tokenization for sentiment analysis of customer reviews for <b>Target</b> (American retailer brand)  scraped from TrustPilot</p>
            <div class="text-button">
              <a href="#nlpsa_open" onclick="document.getElementById('nlpsa').innerHTML = document.getElementById('nlpsa_open').innerHTML">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="nlpsa_open" class="col-lg-3" style="display: none;">
          <div class="service-item second-service">
            <div class="icon"></div>
            <h4>Sentiment Analysis with NLP</h4>
            <p>The transition from numbers to alphabets was a quantum leap for computing. It allowed programmers to go from assembly language that allowed users to simply code in 1's and 0's, to creating methodologies that allowed programmers to code in an easier to understand and a much more transferable language quite similar to english. <br>
              A similar leap in AI was the machine's ability to understand words and map them directly to their meaning. This allowed systems to grasp the "context" of a sentence and thereby predict which word came next. This is the basic structure of language models. Such models are a probabilistic approach to predicting the next word or term in a sequence of words or a sentence. Simply put, language models predict the likelihood of some word that will appear next in a sentence. The overall process here can be broken down into the following steps:<br>
              Understand the word: Many languages contain numerous words that may mean the same thing or to make things more difficult, mean completely different things based on the context. This brings us to the next step<br>
              Understand the context: Different contexts can decide the tone of a sentence, its structure or even its meaning.<br>
              Predict the likelihood: A self explanatory step, but a complex one, predicting the likelihood of the next word involves converting the above two steps into numbers.<br>
              Steps 1 and 2 require a model to understand rather abstract concepts. In the context of this article (hehe, context) abstract refers to any concept that cannot be translated directly into plain and easy-to-understand numbers. These five give rise to qualitative metrics.<br>
              For an AI to make sense of it all, everything has to come down to numbers. That is, in my opinion, the beauty of AI - it can accomplish the most complex tasks possible all by deconstructing these phenomena into plain and simply statistical inference. <br>
              This project aims to utilize the above concept to conduct a sentiment analysis study on customer reviews related to Target, an American retail organization. The reviews under study were scraped from TrustPilot, an online platform where customer reviews are accessible to the public.
              </p>
            <div class="text-button">
              <a href="https://github.com/nupuriyer/target_sentiment_analysis/tree/main" target="_blank">See the full project code <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="nlpcc" class="col-lg-3">
          <div class="service-item third-service">
            <div class="icon"></div>
            <h4>NLP Commonality Clustering</h4>
            <p>Hierarchical clustering for duplicate identification and removal on the CPIN (Child Protection &amp Information Network, Ontario) database</p>
            <div class="text-button">
              <a href="#nlpcc_open" onclick="document.getElementById('nlpcc').innerHTML = document.getElementById('nlpcc_open').innerHTML">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="nlpcc_open" class="col-lg-3" style="display: none;">
          <div class="service-item third-service">
            <div class="icon"></div>
            <h4>NLP Commonality Clustering</h4>
            <p>Child Protection Information Network (CPIN) is a database where child-related agencies upload and track information of all children they have assisted. Records in CPIN should be unique as each record corresponds to exactly one child. However, CPIN is facing a severe issue of duplicate records with the total number of 888K, which is caused by either the data migration of multiple files or multiple uploads of the same child's info from CPIN users. The huge database makes searching difficult, increases maintenance costs and lowers working efficiency.<br>
              The goal of this project is to address the problem of record duplication with an agglomerative clustering mechanism powered by Natural Language Processing. With the help of this mechanism, we aim to flag data duplication across the CPIN database for regular analysis and removal.
              The solution architecture for the above system can be described as follows:<br>
              Bag of Words: The CPIN database stores textual information such as applicant name, address, date of birth, ethnicity, etc. In order to perform clustering, we first preprocess this data to form record documents. This converts every row of the dataset into a “document” identified by row_id.  Following this, we perform a Term Frequency - Inverse Document Frequency (tf-idf) vector that specifies how common a specific term (name, address, any column entry) is to a document (a row).<br>
              Agglomerative Clustering: Once we obtain a tf-idf matrix, we perform agglomerative clustering based on euclidean distance (for maximum explainability) to obtain clusters of rows. These clusters will be formed based on how “similar” a record is to another record, thereby giving us clusters containing the main record and all duplicate records associated with it.<br>
              Golden Record: Once we have a cluster containing all similar records, we mark the record containing the most amount of information (or the least NULL's) as the main record and label all others as duplicates.<br> 
              Monitoring and Maintenance: Monitoring of the system will occur regularly by randomly picking files within the different set Euclidean distance buckets in order to ensure that new files are still being clustered correctly. Further, CPIN workers will be able to adjust distance thresholds if needed. Regular maintenance will be performed on the system to perform potential updates and potential fixes to the system.
              </p>
            <div class="text-button">
              <a href="https://drive.google.com/file/d/1PiaeAjPXjV2JikaFHGHJjdAOlU0OhBGs/view?usp=sharing" target="_blank">Request the full project report <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="candy" class="col-lg-3">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Object Identification: the Candy Counter project</h4>
            <p>Demonstrating the object detection model powered by HuggingFace to create a custom, fine-tuned candy counter model</p>
            <div class="text-button">
              <a href="#candy_open" onclick="document.getElementById('candy').innerHTML = document.getElementById('candy_open').innerHTML">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="candy_open" class="col-lg-3" style="display: none;">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Object Identification: the Candy Counter project</h4>
            <p>The Candy Counter project utilizes Hugging Face's DETR object identification model and fine tunes it to a custom dataset of different types of candy images. The project uses a few-shot learning approach as the final results are obtained using a small dataset for training (~20 images)<br>
              Object detection, a task in computer vision, allows users to create machine systems that can identify not only the number of different objects in a picture frame, but also segregate between the different types of these objects. For instance, such models can be used to either identify the different obstacles in a track or the different types of elements such as vehicles, street signs, etc in a frame.<br>
              The major problem when training object detectors is the availability of data. In order to specify different types of elements in a frame, the model needs access to a repository of pictures of these elements in different lighting, different backgrounds and other such settings. Since it isn't feasible to have access to such data for custom projects, we make use of models already trained on a wide variety of object detection tasks.<br>
              Fine-tuning and transfer learning allow users to utilize pre-existing model architecture and customize it to better fit the requirements of the user.
              The Candy counter project uses this methodology by fine-tuning the DETR model by Huggingface. The project is meant to demonstrate the scalability of the model and allow accurate object detection using a handful of example pictures.
              The entire project can be broken down into three phases:<br>
              Labeling the images: In order to label the different types of candy in a picture, we make use of Label Studio. Since the candy image dataset is small, the labeling process for this project is manual.<br>
              Preprocessing: Since we are fine-tuning to an existing model, we introduce a preprocessing phase to ensure that the input file format matches the input requirements of the DETR model. We will also ensure folder structures are as required for the model to process images correctly<br>
              Fitting & Evaluation: Finally, this phase ensures that we find the best possible hyperparameters to maximize model performance, i.e., ensure high accuracy while trying to minimize resource usage to prioritize efficiency.</p>
            <div class="text-button">
              <a href="https://github.com/nupuriyer/object_identification" target="_blank">See project code <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="vae" class="col-lg-3">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Variational Autoencoder (VAE) & Decoder</h4>
            <p>Probabilistic set-up that allows users to create and evaluate synthetically generated information based on the input dataset.</p>
            <div class="text-button">
              <a href="#vae_open" onclick="document.getElementById('vae').innerHTML = document.getElementById('vae_open').innerHTML">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="vae_open" class="col-lg-3" style="display: none;">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Variational Autoencoder (VAE) & Decoder</h4>
            <p>Anomaly detection is a major application of machine learning that has helped individuals sift through a plethora of information in order to spot the odd element out. It is an accelerated technique of identifying common elements and highlighting the ones that don't fit the 'frame' thus created in the process.<br>
              Variational autoencoder and decoder is a probabilistic set-up that allows users to create and evaluate synthetically generated information based on the input dataset. Simply put, a variational autoencoder tries to mimic the input dataset by fitting on it, while the decoder tries to identify the difference between the real data and that created by the autoencoder. <br>
              Thus for such model setups, the elements that have more data will be encoded better and thus will lead to a higher error when put through the decoder. In other words, elements more common in the dataset will be encoded with higher accuracies and will be tough for the decoder to spot as fake.
              Using the above principle, we can identify anomalies in a series structured data such as a video. By breaking a video into individual frames, and passing them through this autoencoder decoder setup, we can highlight those frames that stand out since they're not like the others. These would be frames where an anomalous behaviour within the context of the video is caught.<br>
              The flow of this project will be as follows:<br>
              Preprocessing: This includes converting the input video into an array of images.<br>
              Fitting the model: We will then build the anomaly model based on the principle specified above and set up a threshold value fine-tuned to the video. This will depend on the input video. For this project the boat_river video is used.</p>
            <div class="text-button">
              <a href="https://github.com/nupuriyer/vae_anomaly" target="_blank">See project code <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="cofi" class="col-lg-3">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Anomaly Detection & Recommender Systems</h4>
            <p>Analysis of the fundamental distribution types applied to datasets & a CoFi Recommender system applied to a movie ratings dataset</p>
            <div class="text-button">
              <a href="#cofi_open" onclick="document.getElementById('cofi').innerHTML = document.getElementById('cofi_open').innerHTML">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="cofi_open" class="col-lg-3" style="display: none;">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Anomaly Detection & Recommender Systems</h4>
            <p>This project is divided into two tasks<br>

              Task 1 comprised of Anomaly Detection and the study and analysis of the fundamental types of Distributions applied to various datasets.<br>
              
              Gaussian Distribution is defined as a continuous function that approximates the exact binomial distribution of events. In order to identify outliers in a data model that is based on such a distribution, we first need to fit the data to its distribution model. We then estimate the mean and variance of the data. Finally, we select the threshold we want to consider and identify the outliers or anomalies.
              <br>
              In order to help us understand how well or poorly we're doing in identifying anomalies for a given threshold, we make use of a concept called the 'F1 Score.'
              In order to calculate the F1 Score, we make use of the following concepts"
              Precision: A ratio of true positives to the total number of examples marked as anomalies
              Recall: A ratio of true positives to the actual number of anomalies
              F1 score is a better measure of accuracy.
              <br>
              Task 2 involved building a Recommender system applied to a movie ratings dataset.
              <br>
              Collaborative Filtering Learning algorithm involves using the data present in order to predict data that is missing. In our dataset, it uses existing ratings for movies given by users and tries to decide ratings that our test user would give, based on factors such as relevance, genre, etc. 
              <br>
              We further improve our results by applying Regularization to the model.
              <br>
              All data was numeric in nature and therefore did not require any formatting/transformation.
              <br>
              The entire project lasted for a week.
              Assignment score on Attempt 1: 100%</p>
            <div class="text-button">
              <a href="https://drive.google.com/file/d/14A7f7LO5qvvcJX-eo9trGa-9s9aN_Uhn/view?usp=sharing" target="_blank">See the full project <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="kpca" class="col-lg-3">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Dimensionality Reduction: K-means & PCA</h4>
            <p>K-means clustering for image compression by top n colours in the image. PCA to reduce models to 3D/2D representations</p>
            <div class="text-button">
              <a href="#kpca_open" onclick="document.getElementById('kpca').innerHTML = document.getElementById('kpca_open').innerHTML">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="kpca_open" class="col-lg-3" style="display: none;">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Dimensionality Reduction: K-means & PCA</h4>
            <p>This project is divided into two tasks:<br>

              Task 1 was meant as a means to understanding the fundamentals of Unsupervised-learning.<br>
              
              Machine Learning can be broadly classified into two categories viz:<br>
              1. Supervised Learning<br>
              2. Unsupervised Learning<br>
              
              The major difference between these categories is that one requires results associated to examples in the training set BEFORE prediction while the other does not require predefined result-labels.
              <br>
              Unsupervised learning is the ML-model that groups together various data points on the basis of similarities between their numerous characteristics, thereby not requiring a result-label be assigned to them.
              Examples of situations where Unsupervised Learning is often used include Grouping of houses in different categories as required, Related news articles on online News-boards, Image Compression, etc.
              <br>
              K-means clustering is an algorithm for implementing Unsupervised Machine Learning. It works on the principle of finding the distance between a centroid (a marker randomly initialized) and a data point and moving the centroid in the direction of the data point closest to it. This process is done repeatedly until the entire dataset is divided and grouped into the defined number of categories.
              <br>
              We then used K-means clustering on a 128x128 colour-image and reconstructed it into a 16x24 image compressed according to the top n colours used in the image.
              <br>
              Task 2 was a study in the process of Principal Component Analysis. 
              <br>
              PCA is a technique for reducing dimensionality of complex datasets, thereby increasing their interpretability while ensuring minimum data loss.
              <br>
              We first applied PCA on a two-dimensional data model and built up from there, ultimately targeting to reduce an N-dimensional model down to 3-D followed by 2-D representations.
              <br>
              This model was then applied to a Faces dataset, wherein each image of a face was reduced to a representation built using only the top 100 principal components, thus reducing dataset size considerably.</p>
            <div class="text-button">
              <a href="https://drive.google.com/file/d/1vzjAi2oqaH0x9468aRu0f9dLtEaFLMh0/view?usp=sharing" target="_blank">See the full project <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="biasvar" class="col-lg-3">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Regularization & Bias vs Variance</h4>
            <p>To study the effects of underfitting and overfitting in a Neural Network. Identifying ways to overcome bias/variance in regression.</p>
            <div class="text-button">
              <a href="#biasvar_open" onclick="document.getElementById('biasvar').innerHTML = document.getElementById('biasvar_open').innerHTML">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="biasvar_open" class="col-lg-3" style="display: none;">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Regularization & Bias vs Variance</h4>
            <p>The fundamental idea behind this project was to study the effects of the two most common problems associated with data analysis in a Neural Network:<br>
              1. High Bias (or Underfitting)<br>
              2. High Variance (or Overfitting)<br>
              
              In order to recreate the scenario of bias, we take into consideration a data that does not follow a linear trend and apply on it a Linear Regression model. As a result, we can clearly identify bias in our model. Furthermore, we can make use of learning curves (Plots resulting as Errors on both Training as well as Test sets as a function of the number of examples used) to identify the exact nature of problem our model may be facing.
              <br>
              Variance is the problem faced when our model overfits the data, i.e. it tries to incorporate almost every data point in consideration, thereby losing the general trend of said data. Overfitting can pose problems for prediction systems. A simple way, as we find, to tell if our model is suffering from a High Variance issue, is if our Training error is far less than our Cross Validation error. 
              <br>
              We try to investigate numerous ways to overcome either problems. Once identification of the exact nature of problem with our training model, we can implement either of the following solutions to overcome them:<br>
              1. Increasing number of examples (Variance)<br>
              2. Reducing feature-set size (Variance)<br>
              3. Adding more features (Bias)<br>
              4. Regularization (Variance)<br>
              5. Adding polynomial features (Bias)<br>
              <br>
              All data was numeric in nature and therefore did not require any formatting/transformation.
              <br>
              The entire project lasted for a week.
              Assignment score on Attempt 1: 100%</p>
            <div class="text-button">
              <a href="https://drive.google.com/file/d/1pnIsWmznCuG5s_ja_ixyZ0T2a67SZJPj/view?usp=sharing" target="_blank">See the full project <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="nn" class="col-lg-3">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Multi-class Classification in Neural Networks</h4>
            <p>Introductory analysis to Neural Networks and multi-class classification with widely used algorithms for Image Recognition.</p>
            <div class="text-button">
              <a href="#nn_open" onclick="document.getElementById('nn').innerHTML=document.getElementById('nn_open').innerHTML;">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="nn_open" class="col-lg-3" style="display: none;">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Multi-class Classification in Neural Networks</h4>
            <p>This project was an introductory analysis to Neural Networks and utilizing the fundamentals of widely used algorithms involved in Image Recognition.<br>

              Logistic Regression is a methodology used for binary-outcome datasets. Examples include Predicting if a tumour is malignant or benign, Whether sea-levels at high tide are expected to go above or below a specified level, etc.<br>
              
              The main purpose of this project was to identify handwritten numbers from given images. In order to achieve this, we used Logistic Regression along combined with the 'one vs all' logic to implement a Multiclass Classification System. We assigned each number (0 to 9) a class and trained the system making use of pixels in every image as the feature set for that example. 
              The other part of this project involved Regularization of the training model. Regularization is defined as the process of adding weight to a particular feature set, thereby effectively reducing its contribution to the final result obtained. It is a technique used to lower Variance or Over-fitting in a training model.
              <br>
              Accuracy obtained: 
              Before Regularization: 95.3%
              After Regularization: 99.56%
              <br>
              All data was image data and therefore was first converted into a numeric format before analysis and model training.
              <br>
              The entire project lasted for a week.
              Assignment score on Attempt 1: 100%</p>
            <div class="text-button">
              <a href="https://drive.google.com/file/d/1hnGVJg2wp0f9l6ezdqIx5moBe2OSZkuS/view?usp=sharing" target="_blank">See the full project <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="svm" class="col-lg-3">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Support Vector Machine Implementation</h4>
            <p>GNU Octave implementation of a Support Vector Machine algorithm (Supervised Learning) for Spam Email identification</p>
            <div class="text-button">
              <a href="#svm_open" onclick="document.getElementById('svm').innerHTML = document.getElementById('svm_open').innerHTML">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="svm_open" class="col-lg-3" style="display: none;">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Support Vector Machine Implementation</h4>
            <p>The main purpose of this project was to create a Support Vector Machine for Spam Email identification.<br> 

              Support Vector Machines are defined as Supervised Learning models that help analyze different types of datasets for regression and/or classification type analysis. For the purpose of this project, we will be perfecting out SVM model by applying it to different Classification-type datasets and finally apply it to filter out spam emails.
              <br>
              We would also be dealing with High Bias and/or High Variance problems, by including in our model Regularization as needed.
              <br>
              Before applying our perfected SVM model, however, we need to pre-process some of the sample emails we'll be working with, thereby grouping certain common features (such as incorrectly spelled words, numbers in place of letters, etc). We will be using these as identifiers and appending them to the Feature list.
              <br>
              Spam Recognition data was not numeric in nature, as it consisted of text samples from emails. Formatting/Transformation was thus required.
              <br>
              Model Accuracy:
              Training Accuracy: 99.825%
              Test Accuracy: 98.8%
              <br>
              The entire project lasted for a week.
              Assignment score on Attempt 1: 100%</p>
            <div class="text-button">
              <a href="https://drive.google.com/file/d/1az9N7u1f7CW3Q23n-VGlri2ROLn0I__6/view?usp=sharing" target="_blank">See the full project <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="log_reg" class="col-lg-3">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Logistic Regression: ML Fundamentals Toolkit</h4>
            <p>Introductory analysis to Machine Learning and the fundamentals of the most widely used algorithms involved in Binary Prediction</p>
            <div class="text-button">
              <a href="#log_reg_open" onclick="
              document.getElementById('log_reg').innerHTML=document.getElementById('log_reg_open').innerHTML;">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="log_reg_open" class="col-lg-3" style="display: none;">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Logistic Regression: ML Fundamentals Toolkit</h4>
            <p>This project was an introductory analysis to Machine Learning and the fundamentals of the most widely used algorithms involved in Binary Prediction.<br>

              Logistic Regression is a methodology used for binary-outcome datasets. Examples include Predicting if a tumour is malignant or benign, Whether sea-levels at high tide are expected to go above or below a specified level, etc.
              The main purpose of this project was to determine the likelihood of a student's admission in a university based on their result in two exams. Essentially, we need to find the direct correlation between two exam results and admission, on the basis of how many students had previously obtained admission.<br>
              Our main objective here is to figure out the decision boundary between admissible and non-admissible students.
              These decision boundaries could be<br>
              1. Linear<br>
              2. Non Linear<br>
              
              It is our job to specify what shape the boundary would take depending upon the type of data available.
              
              All data was numeric in nature and therefore did not require any formatting/transformation.
              <br>
              The entire project lasted for a week.
              Assignment score on Attempt 1: 100%</p>
            <div class="text-button">
              <a href="https://drive.google.com/file/d/1EbItfCO6AEZu8TXHTK-Je8D6T1AlWUas/view?usp=sharing" target="_blank">See the full project <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="lin_reg" class="col-lg-3">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Linear Regression: ML Fundamentals Toolkit</h4>
            <div>
            <p>Introductory analysis to Machine Learning and the fundamentals of the most widely used algorithms involved in Trend Prediction</p></div>
            <div id="lin_reg_but" class="text-button">
              <a href="#lin_reg_open" onclick="
                document.getElementById('lin_reg').innerHTML=document.getElementById('lin_reg_open').innerHTML;">Read More <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>

        <div id="lin_reg_open" class="col-lg-3" style="display: none;">
          <div class="service-item fourth-service">
            <div class="icon"></div>
            <h4>Linear Regression: ML Fundamentals Toolkit</h4>
            <div>
            <p>This project was an introductory analysis to Machine Learning and the fundamentals of the most widely used algorithms involved in Trend Prediction. 

              Linear Regression is a methodology used for continuous datasets. Examples include Weather Prediction, Housing Prices, Market Popularity, etc. <br>
              Task 1) The main purpose of this part of the project was to predict the profit earned for a food truck in direct relation to the population of the city of operation. In other words, identifying the correlation between the result set and data having only one variable (linear trend).
              <br>
              Task 2) This part involved identifying the dependency of the result set on multiple variables. The dataset chosen for this was Housing Prices prediction with the variables as: a. Size of house (in sq ft) and b: Number of bedrooms in the house.
              All data was numeric in nature and therefore did not require any formatting/transformation.
              <br>
              The entire project lasted for a week. 
              Assignment score on Attempt 1: 100%</p></div>
            <div class="text-button">
              <a href="https://drive.google.com/file/d/1p1_LN8r-MRL7Xo4CCo5P3vBF3sgIQuky/view?usp=sharing" target="_blank">See the full project <i class="fa fa-arrow-right"></i></a>
            </div>
          </div>
        </div>
        
        
      </div>
    </div>
  </div>

  <footer id="newsletter">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 offset-lg-2">
          <div class="section-heading">
            <h4>Let's connect online!</h4>
          </div>
        </div>
        <div class="col-lg-6 offset-lg-3">
          <form id="search" action="mailto:nupur99iyer@gmail.com" method="GET">
            <div class="row">
              <div class="col-lg-6 col-sm-6">
                <fieldset>
                  <input type="address" name="address" class="email" placeholder="Type your message here..." autocomplete="on">
                </fieldset>
              </div>
              <div class="col-lg-6 col-sm-6">
                <fieldset>
                  <button type="submit" class="main-button">Shoot me an Email <i class="fa fa-angle-right"></i></button>
                </fieldset>
              </div>
            </div>
          </form>

          <form id="search" action="https://www.linkedin.com/in/nupur-iyer/" method="GET">
            <div class="row">
              <div class="col-lg-6 col-sm-6">
                <fieldset>
                  <input type="address" name="address" class="email" placeholder="Type your message here..." autocomplete="on">
                </fieldset>
              </div>
              <div class="col-lg-6 col-sm-6">
                <fieldset>
                  <button type="submit" class="main-button">Let's connect on LinkedIn <i class="fa fa-angle-right"></i></button>
                </fieldset>
              </div>
            </div>
          </form>
        </div>
      </div>
    </div>
  </footer>


  <!-- Scripts -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/js/owl-carousel.js"></script>
  <script src="assets/js/animation.js"></script>
  <script src="assets/js/imagesloaded.js"></script>
  <script src="assets/js/popup.js"></script>
  <script src="assets/js/custom.js"></script>
  <script>
    // When the user clicks on <div>, open the popup
    function myFunction() {
      var popup = document.getElementById("myPopup");
      popup.classList.toggle("show");
    }
    </script>
</body>
</html>
